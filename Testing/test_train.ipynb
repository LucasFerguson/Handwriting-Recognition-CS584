{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_WIDTH = 28\n",
    "IMAGE_HEIGHT = 28\n",
    "IMAGE_CHANNELS = 1\n",
    "\n",
    "# IMAGE_DIRECTORY = \"../Datasets/HandWritten-Nist/dataset\"\n",
    "IMAGE_DIRECTORY = \"../Datasets/MergedMNIST/dataset\"\n",
    "\n",
    "dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "\tIMAGE_DIRECTORY,\n",
    "\tlabels=\"inferred\",\n",
    "\tlabel_mode=\"categorical\", # one-hot encoding stuff\n",
    "\timage_size=(28, 28),\n",
    "\tshuffle=True,\n",
    "\tcolor_mode=\"grayscale\",\n",
    ")\n",
    "\n",
    "# Split to train and test 80/20\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset = dataset.take(train_size)\n",
    "val_dataset = dataset.skip(train_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the mapping of labels to folder names\n",
    "class_names = dataset.class_names\n",
    "\n",
    "# Print the class names\n",
    "print(\"Class names (labels mapped to folder names):\", class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_me = dataset.take(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "for images, labels in print_me.take(1):\n",
    "\t# show the first 9 images\n",
    "\tfor i in range(9):\n",
    "\t\tplt.subplot(330 + 1 + i)\n",
    "\t\tplt.imshow(images[i].numpy().reshape(28, 28), cmap=plt.get_cmap('gray'))\n",
    "\t\tl = np.argmax(labels[i])\n",
    "\t\tp = class_names[l]\n",
    "\t\tplt.title(f\"Label={p}\")\n",
    "\t\tplt.axis('off')\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the dataset prior to the model\n",
    "\n",
    "def normalize(image, label):\n",
    "\timage = tf.cast(image, tf.float32) / 255.0\n",
    "\treturn image, label\n",
    "\n",
    "train_dataset = train_dataset.map(normalize)\n",
    "val_dataset = val_dataset.map(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Configuring the dataset for performance\n",
    "# # Enable caching and prefetching (overlap preprocessing and model execution while training)\n",
    "# AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "# # Set up a pre-processing function. It is just going to be normalizing here, but\n",
    "# # that hopefully should save time later?\n",
    "# def preprocess_image(image, label):\n",
    "# \timage = image / 255.0\n",
    "# \treturn image, label\n",
    "\n",
    "# # Cache to disk instead of memory because we got some large datasets\n",
    "# dataset = dataset.cache('/tmp/dataset_cache')\n",
    "\n",
    "# # Apply preprocessing\n",
    "# # dataset = dataset.map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "# # Prefetching..? Gotta be honest, this seems to help but idk how considering all\n",
    "# # the images are local\n",
    "# dataset = dataset.prefetch(buffer_size=2)\n",
    "\n",
    "# train_ds = dataset.shuffle(1000)\n",
    "\n",
    "# # train_ds = dataset.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "# # train_ds = dataset.shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "# # val_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "# train_dataset = train_dataset.cache('/tmp/dataset_cache')\n",
    "train_dataset = train_dataset.cache()\n",
    "train_dataset = train_dataset.shuffle(1000)\n",
    "train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "val_dataset = val_dataset.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "# Hypothesis by doing tanh here stems from\n",
    "# 1) Observing that LeNet while simple uses it\n",
    "# 2) The initial layers are looking for general features, not specific ones\n",
    "model.add(tf.keras.layers.Conv2D(\n",
    "    input_shape=(IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_CHANNELS),\n",
    "    kernel_size=5,\n",
    "    filters=8,\n",
    "    strides=1,\n",
    "\tactivation=tf.keras.activations.relu,\n",
    "))\n",
    "\n",
    "# Batch normalizationhelps helps train models faster. It does this in a few\n",
    "# ways. It allows us (1) to increase the learning rate (2) it helps resolve the\n",
    "# vanishing gradient problem somewhat (usually done by ReLU though) (3)\n",
    "# regularization which helps prevent overfitting (we could remove the dropout\n",
    "# layer possibly)\n",
    "# https://www.datacamp.com/tutorial/batch-normalization-tensorflow\n",
    "# https://arxiv.org/abs/1502.03167\n",
    "# https://www.geeksforgeeks.org/what-is-batch-normalization-in-cnn/#how-does-batch-normalization-work-in-cnn\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "\n",
    "model.add(tf.keras.layers.MaxPool2D(\n",
    "    pool_size=(2, 2),\n",
    "    strides=(2, 2)\n",
    "))\n",
    "\n",
    "\n",
    "# Hypothesis by doing relu here is that this layer is going to be looking for\n",
    "# more abstract features and they should be an off/on situation\n",
    "model.add(tf.keras.layers.Conv2D(\n",
    "    kernel_size=5,\n",
    "    filters=16,\n",
    "    strides=1,\n",
    "\tactivation=tf.keras.activations.relu,\n",
    "))\n",
    "\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "model.add(tf.keras.layers.MaxPool2D(\n",
    "    pool_size=(2, 2),\n",
    "    strides=(2, 2)\n",
    "))\n",
    "\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "# LeNet uses 120 neurons, online example uses 128\n",
    "# We will use 200\n",
    "model.add(tf.keras.layers.Dense(\n",
    "    units=200,\n",
    "    activation=tf.keras.activations.relu\n",
    "))\n",
    "\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "# Output layer\n",
    "model.add(tf.keras.layers.Dense(\n",
    "    units=47,\n",
    "    activation=tf.keras.activations.softmax,\n",
    "    kernel_initializer=tf.keras.initializers.VarianceScaling()\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "\n",
    "# Uses Adam. Adam works by\n",
    "# - Keeping track of a history of gradients as opposed to just the current\n",
    "#   gradient\n",
    "# - It uses an adaptive learning rate based on that gradient history Adam is\n",
    "\n",
    "# It also just generally efficient and works well for our use case (ChatGPT\n",
    "# mentions good for computer vision tasks which is kind of what we are doing)\n",
    "\n",
    "# We might try testing this with stochastic gradient descent instead because\n",
    "# that is what we learned in class. We could use that as a storytelling aspect\n",
    "# for the professor\n",
    "\n",
    "# adam_optimizer = tf.keras.optimizers.Adam()\n",
    "# model.compile(\n",
    "#     optimizer=adam_optimizer,\n",
    "#     loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "#     metrics=['accuracy']\n",
    "# )\n",
    "\n",
    "# In past experiments, SGD has performed better and it is apparently used in\n",
    "# other image networks\n",
    "# https://shaoanlu.wordpress.com/2017/05/29/sgd-all-which-one-is-the-best-optimizer-dogs-vs-cats-toy-experiment/\n",
    "\n",
    "sgd_optimizer = tf.keras.optimizers.SGD()\n",
    "model.compile(\n",
    "\toptimizer=sgd_optimizer,\n",
    "\tloss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir=\".logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=1)\n",
    "\n",
    "training_history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=50,\n",
    "    validation_data=val_dataset,\n",
    "\tcallbacks=[early_stop]\n",
    ")\n",
    "\n",
    "print(\"The model has successfully trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access training loss and accuracy\n",
    "train_loss = training_history.history['loss']\n",
    "train_accuracy = training_history.history['accuracy']\n",
    "\n",
    "# Access validation loss and accuracy (if validation data is provided)\n",
    "val_loss = training_history.history.get('val_loss')\n",
    "val_accuracy = training_history.history.get('val_accuracy')\n",
    "\n",
    "# Print statistics for each epoch\n",
    "for epoch in range(len(train_loss)):\n",
    "    print(f\"Epoch {epoch+1}:\")\n",
    "    print(f\"  Training Loss: {train_loss[epoch]:.4f}, Training Accuracy: {train_accuracy[epoch]:.4f}\")\n",
    "    if val_loss and val_accuracy:\n",
    "        print(f\"  Validation Loss: {val_loss[epoch]:.4f}, Validation Accuracy: {val_accuracy[epoch]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_string = \"model_v10_50_epochs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Plot training and validation loss\n",
    "\n",
    "with plt.style.context(\"seaborn-v0_8\"):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(training_history.history['loss'], label='Training Loss')\n",
    "    if 'val_loss' in training_history.history:\n",
    "        plt.plot(training_history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title(f'{model_name_string} - Loss over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot training and validation accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(training_history.history['accuracy'], label='Training Accuracy')\n",
    "    if 'val_accuracy' in training_history.history:\n",
    "        plt.plot(training_history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title(f'{model_name_string} - Accuracy over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    # plt.style.use('seaborn-v0_8-bright')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert the history to a DataFrame and save as CSV\n",
    "history_df = pd.DataFrame(training_history.history)\n",
    "history_df.to_csv(f'{model_name_string}_history.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model itself\n",
    "model.save(f'{model_name_string}.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model weights\n",
    "model.save_weights(f'{model_name_string}.weights.h5') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
